{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<TensorSliceDataset shapes: (), types: tf.string>\n['aeroplane' 'bicycle' 'bird' 'boat' 'bottle' 'bus' 'car' 'cat' 'chair'\n 'cow' 'diningtable' 'dog' 'horse' 'motorbike' 'person' 'pottedplant'\n 'sheep' 'sofa' 'train' 'tvmonitor']\n(5834,)\n<TensorSliceDataset shapes: (20,), types: tf.float32>\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Create dataset of image paths in train folder\n",
    "paths_ds = tf.data.Dataset.list_files(str(\"./VOC2007_train_subimages/*.jpg\"), shuffle=False)\n",
    "print(paths_ds)\n",
    "\n",
    "# Helpfer function to extract class/label names from paths\n",
    "def classnamefromPath(path):\n",
    "    parts = tf.strings.split(path, os.sep)\n",
    "    filename = parts[-1]\n",
    "    label = tf.strings.split(filename, '_')[-1]\n",
    "    label = tf.strings.split(label, '.')[0]\n",
    "\n",
    "    return label\n",
    "\n",
    "# Create list of label/class names\n",
    "labels = paths_ds.map(classnamefromPath)\n",
    "labels = np.array(list(labels.as_numpy_iterator()))\n",
    "labels = [label.decode('utf-8') for label in labels]\n",
    "label_names = np.unique(labels)\n",
    "print(label_names)\n",
    "\n",
    "# Helper fuction to convert string labels to class number\n",
    "def indexfromClassname(name, labels):\n",
    "    idx = np.where(labels == name)\n",
    "    return idx[0][0]\n",
    "\n",
    "# Create list of class indices\n",
    "labels_indices = [indexfromClassname(label, label_names) for label in labels]\n",
    "labels_indices = np.array(labels_indices)\n",
    "print(labels_indices.shape)\n",
    "\n",
    "# Encode class indices in one-hot form\n",
    "target_ds = tf.one_hot(labels_indices, len(label_names), on_value=1.0, off_value=0.0)\n",
    "target_ds = tf.data.Dataset.from_tensor_slices(target_ds)\n",
    "print(target_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to read images and pre-process images\n",
    "# InceptionV3 accepts input of size (299,299,3)\n",
    "# The input range is (-1,1)\n",
    "def readImages(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "    image = tf.image.resize_with_pad(image, 299, 299)\n",
    "    image = keras.applications.inception_v3.preprocess_input(image)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Path: tf.Tensor(b'.\\\\VOC2007_train_subimages\\\\000005_chair.jpg', shape=(), dtype=string)\nLabel: chair\n(299, 299, 3)\n1.0\n-1.0\n"
    }
   ],
   "source": [
    "# print some information to check on images, label and target\n",
    "path = next(iter(paths_ds))\n",
    "image = readImages(path)\n",
    "\n",
    "print(\"Path: \" + str(path))\n",
    "print(\"Label: \" + str(next(iter(labels))))\n",
    "print(image.shape)\n",
    "print(np.max(image))\n",
    "print(np.min(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<MapDataset shapes: (299, 299, None), types: tf.float32>\n"
    }
   ],
   "source": [
    "# Create dataset of input data, pre-processed images and target\n",
    "images_ds = paths_ds.map(readImages)\n",
    "print(images_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<BatchDataset shapes: ((None, 299, 299, None), (None, 20)), types: (tf.float32, tf.float32)>\n"
    }
   ],
   "source": [
    "# Create main dataset from input and target datasets\n",
    "train_dataset = tf.data.Dataset.zip((images_ds, target_ds))\n",
    "train_dataset = train_dataset.batch(8)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 730 steps\n730/730 [==============================] - 892s 1s/step - loss: 1.4141 - categorical_accuracy: 0.6109\n"
    }
   ],
   "source": [
    "# Create pre-trained model\n",
    "inceptionv3 = keras.applications.inception_v3.InceptionV3(weights='imagenet', input_shape=(299, 299, 3), include_top=False)\n",
    "\n",
    "# Add global spatial average pooling layer\n",
    "x = inceptionv3.output\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a fully-connected layer to the raw output of network\n",
    "x = keras.layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add a drop-out layer\n",
    "x = keras.layers.Dropout(rate=0.5)(x)\n",
    "\n",
    "# Add a logistic layer (softmax) - to predict 10 classes\n",
    "predictions = keras.layers.Dense(20, activation='softmax')(x)\n",
    "\n",
    "# Compose the model based on new top-layer\n",
    "new_inceptionv3 = keras.models.Model(inputs=inceptionv3.input, outputs=predictions)\n",
    "\n",
    "# First: Train the new top-layer only\n",
    "# Hence, freeze all layers in pre-trained model\n",
    "for layer in inceptionv3.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model, ready to be trained on new data\n",
    "new_inceptionv3.compile(optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9, decay=0.0001), loss=keras.losses.CategoricalCrossentropy(), metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "# Train the top-layer only\n",
    "new_inceptionv3.fit(train_dataset, epochs=1)\n",
    "new_inceptionv3.save('inceptionv3_pascalvoc_0_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 input_1\n1 conv2d\n2 batch_normalization\n3 activation\n4 conv2d_1\n5 batch_normalization_1\n6 activation_1\n7 conv2d_2\n8 batch_normalization_2\n9 activation_2\n10 max_pooling2d\n11 conv2d_3\n12 batch_normalization_3\n13 activation_3\n14 conv2d_4\n15 batch_normalization_4\n16 activation_4\n17 max_pooling2d_1\n18 conv2d_8\n19 batch_normalization_8\n20 activation_8\n21 conv2d_6\n22 conv2d_9\n23 batch_normalization_6\n24 batch_normalization_9\n25 activation_6\n26 activation_9\n27 average_pooling2d\n28 conv2d_5\n29 conv2d_7\n30 conv2d_10\n31 conv2d_11\n32 batch_normalization_5\n33 batch_normalization_7\n34 batch_normalization_10\n35 batch_normalization_11\n36 activation_5\n37 activation_7\n38 activation_10\n39 activation_11\n40 mixed0\n41 conv2d_15\n42 batch_normalization_15\n43 activation_15\n44 conv2d_13\n45 conv2d_16\n46 batch_normalization_13\n47 batch_normalization_16\n48 activation_13\n49 activation_16\n50 average_pooling2d_1\n51 conv2d_12\n52 conv2d_14\n53 conv2d_17\n54 conv2d_18\n55 batch_normalization_12\n56 batch_normalization_14\n57 batch_normalization_17\n58 batch_normalization_18\n59 activation_12\n60 activation_14\n61 activation_17\n62 activation_18\n63 mixed1\n64 conv2d_22\n65 batch_normalization_22\n66 activation_22\n67 conv2d_20\n68 conv2d_23\n69 batch_normalization_20\n70 batch_normalization_23\n71 activation_20\n72 activation_23\n73 average_pooling2d_2\n74 conv2d_19\n75 conv2d_21\n76 conv2d_24\n77 conv2d_25\n78 batch_normalization_19\n79 batch_normalization_21\n80 batch_normalization_24\n81 batch_normalization_25\n82 activation_19\n83 activation_21\n84 activation_24\n85 activation_25\n86 mixed2\n87 conv2d_27\n88 batch_normalization_27\n89 activation_27\n90 conv2d_28\n91 batch_normalization_28\n92 activation_28\n93 conv2d_26\n94 conv2d_29\n95 batch_normalization_26\n96 batch_normalization_29\n97 activation_26\n98 activation_29\n99 max_pooling2d_2\n100 mixed3\n101 conv2d_34\n102 batch_normalization_34\n103 activation_34\n104 conv2d_35\n105 batch_normalization_35\n106 activation_35\n107 conv2d_31\n108 conv2d_36\n109 batch_normalization_31\n110 batch_normalization_36\n111 activation_31\n112 activation_36\n113 conv2d_32\n114 conv2d_37\n115 batch_normalization_32\n116 batch_normalization_37\n117 activation_32\n118 activation_37\n119 average_pooling2d_3\n120 conv2d_30\n121 conv2d_33\n122 conv2d_38\n123 conv2d_39\n124 batch_normalization_30\n125 batch_normalization_33\n126 batch_normalization_38\n127 batch_normalization_39\n128 activation_30\n129 activation_33\n130 activation_38\n131 activation_39\n132 mixed4\n133 conv2d_44\n134 batch_normalization_44\n135 activation_44\n136 conv2d_45\n137 batch_normalization_45\n138 activation_45\n139 conv2d_41\n140 conv2d_46\n141 batch_normalization_41\n142 batch_normalization_46\n143 activation_41\n144 activation_46\n145 conv2d_42\n146 conv2d_47\n147 batch_normalization_42\n148 batch_normalization_47\n149 activation_42\n150 activation_47\n151 average_pooling2d_4\n152 conv2d_40\n153 conv2d_43\n154 conv2d_48\n155 conv2d_49\n156 batch_normalization_40\n157 batch_normalization_43\n158 batch_normalization_48\n159 batch_normalization_49\n160 activation_40\n161 activation_43\n162 activation_48\n163 activation_49\n164 mixed5\n165 conv2d_54\n166 batch_normalization_54\n167 activation_54\n168 conv2d_55\n169 batch_normalization_55\n170 activation_55\n171 conv2d_51\n172 conv2d_56\n173 batch_normalization_51\n174 batch_normalization_56\n175 activation_51\n176 activation_56\n177 conv2d_52\n178 conv2d_57\n179 batch_normalization_52\n180 batch_normalization_57\n181 activation_52\n182 activation_57\n183 average_pooling2d_5\n184 conv2d_50\n185 conv2d_53\n186 conv2d_58\n187 conv2d_59\n188 batch_normalization_50\n189 batch_normalization_53\n190 batch_normalization_58\n191 batch_normalization_59\n192 activation_50\n193 activation_53\n194 activation_58\n195 activation_59\n196 mixed6\n197 conv2d_64\n198 batch_normalization_64\n199 activation_64\n200 conv2d_65\n201 batch_normalization_65\n202 activation_65\n203 conv2d_61\n204 conv2d_66\n205 batch_normalization_61\n206 batch_normalization_66\n207 activation_61\n208 activation_66\n209 conv2d_62\n210 conv2d_67\n211 batch_normalization_62\n212 batch_normalization_67\n213 activation_62\n214 activation_67\n215 average_pooling2d_6\n216 conv2d_60\n217 conv2d_63\n218 conv2d_68\n219 conv2d_69\n220 batch_normalization_60\n221 batch_normalization_63\n222 batch_normalization_68\n223 batch_normalization_69\n224 activation_60\n225 activation_63\n226 activation_68\n227 activation_69\n228 mixed7\n229 conv2d_72\n230 batch_normalization_72\n231 activation_72\n232 conv2d_73\n233 batch_normalization_73\n234 activation_73\n235 conv2d_70\n236 conv2d_74\n237 batch_normalization_70\n238 batch_normalization_74\n239 activation_70\n240 activation_74\n241 conv2d_71\n242 conv2d_75\n243 batch_normalization_71\n244 batch_normalization_75\n245 activation_71\n246 activation_75\n247 max_pooling2d_3\n248 mixed8\n249 conv2d_80\n250 batch_normalization_80\n251 activation_80\n252 conv2d_77\n253 conv2d_81\n254 batch_normalization_77\n255 batch_normalization_81\n256 activation_77\n257 activation_81\n258 conv2d_78\n259 conv2d_79\n260 conv2d_82\n261 conv2d_83\n262 average_pooling2d_7\n263 conv2d_76\n264 batch_normalization_78\n265 batch_normalization_79\n266 batch_normalization_82\n267 batch_normalization_83\n268 conv2d_84\n269 batch_normalization_76\n270 activation_78\n271 activation_79\n272 activation_82\n273 activation_83\n274 batch_normalization_84\n275 activation_76\n276 mixed9_0\n277 concatenate\n278 activation_84\n279 mixed9\n280 conv2d_89\n281 batch_normalization_89\n282 activation_89\n283 conv2d_86\n284 conv2d_90\n285 batch_normalization_86\n286 batch_normalization_90\n287 activation_86\n288 activation_90\n289 conv2d_87\n290 conv2d_88\n291 conv2d_91\n292 conv2d_92\n293 average_pooling2d_8\n294 conv2d_85\n295 batch_normalization_87\n296 batch_normalization_88\n297 batch_normalization_91\n298 batch_normalization_92\n299 conv2d_93\n300 batch_normalization_85\n301 activation_87\n302 activation_88\n303 activation_91\n304 activation_92\n305 batch_normalization_93\n306 activation_85\n307 mixed9_1\n308 concatenate_1\n309 activation_93\n310 mixed10\nTrain for 730 steps\n730/730 [==============================] - 1061s 1s/step - loss: 0.6578 - categorical_accuracy: 0.8097\n"
    }
   ],
   "source": [
    "# Now start fine-tuning the convolutional layers, freeze the bottom N layers and train the remaining top layers\n",
    "# But first, lets visualise the layers available in Inception v3\n",
    "for i, layer in enumerate(inceptionv3.layers):\n",
    "    print(i, layer.name)\n",
    "    \n",
    "# Choose N layers to freeze and unfreeze the rest\n",
    "N = 249\n",
    "for layer in new_inceptionv3.layers[:N]:\n",
    "    layer.trainable = False\n",
    "for layer in new_inceptionv3.layers[N:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Recompile model for modifications to take effect\n",
    "# Use Stochastic Gradient Descent with a Low learning rate this time\n",
    "new_inceptionv3.compile(optimizer=keras.optimizers.SGD(lr=0.0001, momentum=0.9, decay=0.00001), loss=keras.losses.CategoricalCrossentropy(), metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "# Train the model again, this time fine-tuning some inception blocks alongside new top layers\n",
    "new_inceptionv3.fit(train_dataset, epochs=1)\n",
    "new_inceptionv3.save('inceptionv3_pascalvoc_0_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}