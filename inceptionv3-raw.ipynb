{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Images for Transfer Learning\n",
    "\n",
    "Images has to be labelled and bias should be minimised. Using these datasets but only the relevant classes are taken:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "def copyImages(file_str, src, dest):\n",
    "    file = pd.read_csv(file_str, delim_whitespace=True)\n",
    "    \n",
    "    for index, row in file.iterrows():\n",
    "        if(row[1] == 1):\n",
    "            shutil.copyfile((src + str(row[0]) + \".jpg\"),(dest + str(row[0])) + \".jpg\")\n",
    "            \n",
    "copyImages(\"aeroplane_trainval.txt\", \"JPEGImages/\", \"aeroplane/\")\n",
    "\n",
    "copyImages(\"bicycle_trainval.txt\", \"JPEGImages/\", \"bicycle/\")\n",
    "\n",
    "copyImages(\"bird_trainval.txt\", \"JPEGImages/\", \"bird/\")\n",
    "\n",
    "copyImages(\"boat_trainval.txt\", \"JPEGImages/\", \"boat/\")\n",
    "\n",
    "copyImages(\"bottle_trainval.txt\", \"JPEGImages/\", \"bottle/\")\n",
    "\n",
    "copyImages(\"cat_trainval.txt\", \"JPEGImages/\", \"cat/\")\n",
    "\n",
    "copyImages(\"chair_trainval.txt\", \"JPEGImages/\", \"chair/\")\n",
    "\n",
    "copyImages(\"diningtable_trainval.txt\", \"JPEGImages/\", \"diningtable/\")\n",
    "\n",
    "copyImages(\"person_trainval.txt\", \"JPEGImages/\", \"person/\")\n",
    "\n",
    "copyImages(\"pottedplant_trainval.txt\", \"JPEGImages/\", \"pottedplant/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'cat', 'chair', 'diningtable', 'person', 'pottedplant']\n",
      "image shape:  (299, 299, 3)\n",
      "label shape:  ()\n",
      "types:  (tf.float32, tf.int64)\n",
      "<DatasetV1Adapter shapes: ((299, 299, 3), ()), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "data_root = pathlib.Path(\"./data\")\n",
    "\n",
    "import random\n",
    "\n",
    "all_image_paths = list(data_root.glob('*/*'))\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n",
    "random.shuffle(all_image_paths)\n",
    "image_count = len(all_image_paths)\n",
    "\n",
    "label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "print(label_names)\n",
    "label_to_index = dict((name, index) for index,name in enumerate(label_names))\n",
    "all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
    "                    for path in all_image_paths]\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize_images(image, [299, 299])\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    image -= 0.5\n",
    "    image *= 2.\n",
    "    \n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.read_file(path)\n",
    "    return preprocess_image(image)\n",
    "\n",
    "path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
    "image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(all_image_labels, tf.int64))\n",
    "image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "\n",
    "print('image shape: ', image_label_ds.output_shapes[0])\n",
    "print('label shape: ', image_label_ds.output_shapes[1])\n",
    "print('types: ', image_label_ds.output_types)\n",
    "print(image_label_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Setting a shuffle buffer size as large as the dataset ensures that the data is\n",
    "# completely shuffled.\n",
    "ds = image_label_ds.shuffle(buffer_size=image_count)\n",
    "ds = ds.repeat()\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "# `prefetch` lets the dataset fetch batches, in the background while the model is training.\n",
    "ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "steps_per_epoch = int(np.ceil(len(all_image_paths)/BATCH_SIZE))\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abdul\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\abdul\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Create pre-trained model\n",
    "inceptionv3 = keras.applications.inception_v3.InceptionV3(weights='imagenet', input_shape=(299, 299, 3), include_top=False)\n",
    "\n",
    "# Add global spatial average pooling layer\n",
    "x = inceptionv3.output\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a fully-connected layer to the raw output of network\n",
    "x = keras.layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add a drop-out layer\n",
    "x = keras.layers.Dropout(rate=0.5)(x)\n",
    "\n",
    "# Add a logistic layer (softmax) - to predict 10 classes\n",
    "predictions = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Compose the model based on new top-layer\n",
    "new_inceptionv3 = keras.models.Model(inputs=inceptionv3.input, outputs=predictions)\n",
    "\n",
    "# First: Train the new top-layer only\n",
    "# Hence, freeze all layers in pre-trained model\n",
    "for layer in inceptionv3.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model, ready to be trained on new data\n",
    "new_inceptionv3.compile(optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9, decay=0.0001), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abdul\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "330/330 [==============================] - 1544s 5s/step - loss: 135.1009\n",
      "0 input_1\n",
      "1 conv2d\n",
      "2 batch_normalization_v1\n",
      "3 activation\n",
      "4 conv2d_1\n",
      "5 batch_normalization_v1_1\n",
      "6 activation_1\n",
      "7 conv2d_2\n",
      "8 batch_normalization_v1_2\n",
      "9 activation_2\n",
      "10 max_pooling2d\n",
      "11 conv2d_3\n",
      "12 batch_normalization_v1_3\n",
      "13 activation_3\n",
      "14 conv2d_4\n",
      "15 batch_normalization_v1_4\n",
      "16 activation_4\n",
      "17 max_pooling2d_1\n",
      "18 conv2d_8\n",
      "19 batch_normalization_v1_8\n",
      "20 activation_8\n",
      "21 conv2d_6\n",
      "22 conv2d_9\n",
      "23 batch_normalization_v1_6\n",
      "24 batch_normalization_v1_9\n",
      "25 activation_6\n",
      "26 activation_9\n",
      "27 average_pooling2d\n",
      "28 conv2d_5\n",
      "29 conv2d_7\n",
      "30 conv2d_10\n",
      "31 conv2d_11\n",
      "32 batch_normalization_v1_5\n",
      "33 batch_normalization_v1_7\n",
      "34 batch_normalization_v1_10\n",
      "35 batch_normalization_v1_11\n",
      "36 activation_5\n",
      "37 activation_7\n",
      "38 activation_10\n",
      "39 activation_11\n",
      "40 mixed0\n",
      "41 conv2d_15\n",
      "42 batch_normalization_v1_15\n",
      "43 activation_15\n",
      "44 conv2d_13\n",
      "45 conv2d_16\n",
      "46 batch_normalization_v1_13\n",
      "47 batch_normalization_v1_16\n",
      "48 activation_13\n",
      "49 activation_16\n",
      "50 average_pooling2d_1\n",
      "51 conv2d_12\n",
      "52 conv2d_14\n",
      "53 conv2d_17\n",
      "54 conv2d_18\n",
      "55 batch_normalization_v1_12\n",
      "56 batch_normalization_v1_14\n",
      "57 batch_normalization_v1_17\n",
      "58 batch_normalization_v1_18\n",
      "59 activation_12\n",
      "60 activation_14\n",
      "61 activation_17\n",
      "62 activation_18\n",
      "63 mixed1\n",
      "64 conv2d_22\n",
      "65 batch_normalization_v1_22\n",
      "66 activation_22\n",
      "67 conv2d_20\n",
      "68 conv2d_23\n",
      "69 batch_normalization_v1_20\n",
      "70 batch_normalization_v1_23\n",
      "71 activation_20\n",
      "72 activation_23\n",
      "73 average_pooling2d_2\n",
      "74 conv2d_19\n",
      "75 conv2d_21\n",
      "76 conv2d_24\n",
      "77 conv2d_25\n",
      "78 batch_normalization_v1_19\n",
      "79 batch_normalization_v1_21\n",
      "80 batch_normalization_v1_24\n",
      "81 batch_normalization_v1_25\n",
      "82 activation_19\n",
      "83 activation_21\n",
      "84 activation_24\n",
      "85 activation_25\n",
      "86 mixed2\n",
      "87 conv2d_27\n",
      "88 batch_normalization_v1_27\n",
      "89 activation_27\n",
      "90 conv2d_28\n",
      "91 batch_normalization_v1_28\n",
      "92 activation_28\n",
      "93 conv2d_26\n",
      "94 conv2d_29\n",
      "95 batch_normalization_v1_26\n",
      "96 batch_normalization_v1_29\n",
      "97 activation_26\n",
      "98 activation_29\n",
      "99 max_pooling2d_2\n",
      "100 mixed3\n",
      "101 conv2d_34\n",
      "102 batch_normalization_v1_34\n",
      "103 activation_34\n",
      "104 conv2d_35\n",
      "105 batch_normalization_v1_35\n",
      "106 activation_35\n",
      "107 conv2d_31\n",
      "108 conv2d_36\n",
      "109 batch_normalization_v1_31\n",
      "110 batch_normalization_v1_36\n",
      "111 activation_31\n",
      "112 activation_36\n",
      "113 conv2d_32\n",
      "114 conv2d_37\n",
      "115 batch_normalization_v1_32\n",
      "116 batch_normalization_v1_37\n",
      "117 activation_32\n",
      "118 activation_37\n",
      "119 average_pooling2d_3\n",
      "120 conv2d_30\n",
      "121 conv2d_33\n",
      "122 conv2d_38\n",
      "123 conv2d_39\n",
      "124 batch_normalization_v1_30\n",
      "125 batch_normalization_v1_33\n",
      "126 batch_normalization_v1_38\n",
      "127 batch_normalization_v1_39\n",
      "128 activation_30\n",
      "129 activation_33\n",
      "130 activation_38\n",
      "131 activation_39\n",
      "132 mixed4\n",
      "133 conv2d_44\n",
      "134 batch_normalization_v1_44\n",
      "135 activation_44\n",
      "136 conv2d_45\n",
      "137 batch_normalization_v1_45\n",
      "138 activation_45\n",
      "139 conv2d_41\n",
      "140 conv2d_46\n",
      "141 batch_normalization_v1_41\n",
      "142 batch_normalization_v1_46\n",
      "143 activation_41\n",
      "144 activation_46\n",
      "145 conv2d_42\n",
      "146 conv2d_47\n",
      "147 batch_normalization_v1_42\n",
      "148 batch_normalization_v1_47\n",
      "149 activation_42\n",
      "150 activation_47\n",
      "151 average_pooling2d_4\n",
      "152 conv2d_40\n",
      "153 conv2d_43\n",
      "154 conv2d_48\n",
      "155 conv2d_49\n",
      "156 batch_normalization_v1_40\n",
      "157 batch_normalization_v1_43\n",
      "158 batch_normalization_v1_48\n",
      "159 batch_normalization_v1_49\n",
      "160 activation_40\n",
      "161 activation_43\n",
      "162 activation_48\n",
      "163 activation_49\n",
      "164 mixed5\n",
      "165 conv2d_54\n",
      "166 batch_normalization_v1_54\n",
      "167 activation_54\n",
      "168 conv2d_55\n",
      "169 batch_normalization_v1_55\n",
      "170 activation_55\n",
      "171 conv2d_51\n",
      "172 conv2d_56\n",
      "173 batch_normalization_v1_51\n",
      "174 batch_normalization_v1_56\n",
      "175 activation_51\n",
      "176 activation_56\n",
      "177 conv2d_52\n",
      "178 conv2d_57\n",
      "179 batch_normalization_v1_52\n",
      "180 batch_normalization_v1_57\n",
      "181 activation_52\n",
      "182 activation_57\n",
      "183 average_pooling2d_5\n",
      "184 conv2d_50\n",
      "185 conv2d_53\n",
      "186 conv2d_58\n",
      "187 conv2d_59\n",
      "188 batch_normalization_v1_50\n",
      "189 batch_normalization_v1_53\n",
      "190 batch_normalization_v1_58\n",
      "191 batch_normalization_v1_59\n",
      "192 activation_50\n",
      "193 activation_53\n",
      "194 activation_58\n",
      "195 activation_59\n",
      "196 mixed6\n",
      "197 conv2d_64\n",
      "198 batch_normalization_v1_64\n",
      "199 activation_64\n",
      "200 conv2d_65\n",
      "201 batch_normalization_v1_65\n",
      "202 activation_65\n",
      "203 conv2d_61\n",
      "204 conv2d_66\n",
      "205 batch_normalization_v1_61\n",
      "206 batch_normalization_v1_66\n",
      "207 activation_61\n",
      "208 activation_66\n",
      "209 conv2d_62\n",
      "210 conv2d_67\n",
      "211 batch_normalization_v1_62\n",
      "212 batch_normalization_v1_67\n",
      "213 activation_62\n",
      "214 activation_67\n",
      "215 average_pooling2d_6\n",
      "216 conv2d_60\n",
      "217 conv2d_63\n",
      "218 conv2d_68\n",
      "219 conv2d_69\n",
      "220 batch_normalization_v1_60\n",
      "221 batch_normalization_v1_63\n",
      "222 batch_normalization_v1_68\n",
      "223 batch_normalization_v1_69\n",
      "224 activation_60\n",
      "225 activation_63\n",
      "226 activation_68\n",
      "227 activation_69\n",
      "228 mixed7\n",
      "229 conv2d_72\n",
      "230 batch_normalization_v1_72\n",
      "231 activation_72\n",
      "232 conv2d_73\n",
      "233 batch_normalization_v1_73\n",
      "234 activation_73\n",
      "235 conv2d_70\n",
      "236 conv2d_74\n",
      "237 batch_normalization_v1_70\n",
      "238 batch_normalization_v1_74\n",
      "239 activation_70\n",
      "240 activation_74\n",
      "241 conv2d_71\n",
      "242 conv2d_75\n",
      "243 batch_normalization_v1_71\n",
      "244 batch_normalization_v1_75\n",
      "245 activation_71\n",
      "246 activation_75\n",
      "247 max_pooling2d_3\n",
      "248 mixed8\n",
      "249 conv2d_80\n",
      "250 batch_normalization_v1_80\n",
      "251 activation_80\n",
      "252 conv2d_77\n",
      "253 conv2d_81\n",
      "254 batch_normalization_v1_77\n",
      "255 batch_normalization_v1_81\n",
      "256 activation_77\n",
      "257 activation_81\n",
      "258 conv2d_78\n",
      "259 conv2d_79\n",
      "260 conv2d_82\n",
      "261 conv2d_83\n",
      "262 average_pooling2d_7\n",
      "263 conv2d_76\n",
      "264 batch_normalization_v1_78\n",
      "265 batch_normalization_v1_79\n",
      "266 batch_normalization_v1_82\n",
      "267 batch_normalization_v1_83\n",
      "268 conv2d_84\n",
      "269 batch_normalization_v1_76\n",
      "270 activation_78\n",
      "271 activation_79\n",
      "272 activation_82\n",
      "273 activation_83\n",
      "274 batch_normalization_v1_84\n",
      "275 activation_76\n",
      "276 mixed9_0\n",
      "277 concatenate\n",
      "278 activation_84\n",
      "279 mixed9\n",
      "280 conv2d_89\n",
      "281 batch_normalization_v1_89\n",
      "282 activation_89\n",
      "283 conv2d_86\n",
      "284 conv2d_90\n",
      "285 batch_normalization_v1_86\n",
      "286 batch_normalization_v1_90\n",
      "287 activation_86\n",
      "288 activation_90\n",
      "289 conv2d_87\n",
      "290 conv2d_88\n",
      "291 conv2d_91\n",
      "292 conv2d_92\n",
      "293 average_pooling2d_8\n",
      "294 conv2d_85\n",
      "295 batch_normalization_v1_87\n",
      "296 batch_normalization_v1_88\n",
      "297 batch_normalization_v1_91\n",
      "298 batch_normalization_v1_92\n",
      "299 conv2d_93\n",
      "300 batch_normalization_v1_85\n",
      "301 activation_87\n",
      "302 activation_88\n",
      "303 activation_91\n",
      "304 activation_92\n",
      "305 batch_normalization_v1_93\n",
      "306 activation_85\n",
      "307 mixed9_1\n",
      "308 concatenate_1\n",
      "309 activation_93\n",
      "310 mixed10\n",
      "330/330 [==============================] - 1721s 5s/step - loss: 130.3992\n"
     ]
    }
   ],
   "source": [
    "#new_inceptionv3 = keras.models.load_model('inceptionv3_10classes_1.h5')\n",
    "\n",
    "# Train the model\n",
    "new_inceptionv3.fit(ds, epochs=1, steps_per_epoch=steps_per_epoch)\n",
    "new_inceptionv3.save('inceptionv3_10classes_1.h5')\n",
    "\n",
    "# Top-layers are trained\n",
    "# Now start fine-tuning the convolutional layers, freeze the bottom N layers and train the remaining top layers\n",
    "# But first, lets visualise the layers available in Inception v3\n",
    "for i, layer in enumerate(inceptionv3.layers):\n",
    "    print(i, layer.name)\n",
    "    \n",
    "# Choose N layers to freeze and unfreeze the rest\n",
    "N = 249\n",
    "for layer in new_inceptionv3.layers[:N]:\n",
    "    layer.trainable = False\n",
    "for layer in new_inceptionv3.layers[N:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Recompile model for modifications to take effect\n",
    "# Use Stochastic Gradient Descent with a Low learning rate this time\n",
    "new_inceptionv3.compile(optimizer=keras.optimizers.SGD(lr=0.0001, momentum=0.9, decay=0.00001), loss='categorical_crossentropy')\n",
    "\n",
    "# Train the model again, this time fine-tuning some inception blocks alongside new top layers\n",
    "new_inceptionv3.fit(ds, epochs=1, steps_per_epoch=steps_per_epoch)\n",
    "new_inceptionv3.save('inceptionv3_10classes_1_fine.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-off Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims_4:0\", shape=(1, 299, 299, 3), dtype=float32)\n",
      "['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'cat', 'chair', 'diningtable', 'person', 'pottedplant']\n",
      "[[0.09999993 0.09999993 0.10000122 0.09999993 0.09999983 0.09999979\n",
      "  0.0999998  0.09999982 0.09999987 0.09999991]]\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('inceptionv3_10classes_1.h5')\n",
    "image = load_and_preprocess_image('test-data/bird.jpeg')\n",
    "image = tf.expand_dims(image, axis=0)\n",
    "print(image)\n",
    "results = model.predict(image, batch_size=None, steps=1)\n",
    "print(label_names)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
